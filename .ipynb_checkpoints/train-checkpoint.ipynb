{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "from itertools import repeat\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from data_manager import DataManager, load_img, list_images\n",
    "from data_generator import CustomDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    concatenate,\n",
    "    Convolution2D,\n",
    "    Flatten,\n",
    "    BatchNormalization,\n",
    "    Dropout,\n",
    "    Conv2D,\n",
    "    UpSampling2D,\n",
    "    ELU,\n",
    "    Dense\n",
    ")\n",
    "import tensorflow\n",
    "\n",
    "import tensorflow.keras.activations as activations\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_devices = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tensorflow.config.experimental.set_memory_growth(device, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = os.getcwd()\n",
    "manager = DataManager()\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "\n",
    "def resize_with_padding(img, expected_size):\n",
    "    \"\"\"\n",
    "    this function only works when scaling UP i.e. left,right,top,bottom > 0\n",
    "    \"\"\"\n",
    "    desired_size = expected_size\n",
    "    height, width = img.shape[:2]\n",
    "    delta_width = desired_size[1] - width\n",
    "    delta_height = desired_size[0] - height\n",
    "    pad_width = delta_width // 2\n",
    "    pad_height = delta_height // 2\n",
    "    left, top, right, bottom = (pad_width, pad_height, delta_width - pad_width, delta_height - pad_height)\n",
    "    img = cv2.resize(img, expected_size)\n",
    "    color = [0,0,0]\n",
    "    new_img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT,\n",
    "        value=color)\n",
    "    return new_img\n",
    "\n",
    "def grays_to_RGB(img):\n",
    "    # turn image into grayscale RGB\n",
    "    return np.array(Image.fromarray(img).convert(\"RGB\"))\n",
    "\n",
    "def save_img(img, img_idx, path, pid, is_mask=False):\n",
    "    filename = path + '/' + str(pid) + '_' + str(img_idx) \n",
    "    if is_mask: \n",
    "        filename += '_mask.png' \n",
    "        img = np.asarray(img, dtype=\"uint8\") # convert bool mask into uint8 so cv2 doesn't scream\n",
    "    else:\n",
    "        filename += '.png'\n",
    "        img = grays_to_RGB(img)\n",
    "    \n",
    "    cv2.imwrite(filename, img)\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (f\"Creation of the directory {path} failed\", end='\\r')\n",
    "\n",
    "def gen_dataset(imgs, dataset, pid, labels=None, typeof_dataset=None):\n",
    "    output_dir = BASE + '/data/'+dataset+'/'\n",
    "    if os.path.isdir(output_dir) is False:\n",
    "        make_dir(output_dir)\n",
    "    if typeof_dataset is not None: # this is only for train\n",
    "        output_dir+=typeof_dataset #+ '/'\n",
    "        if os.path.isdir(output_dir) is False:\n",
    "            make_dir(output_dir)\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        save_img(img, i, output_dir, pid)\n",
    "        if labels is not None: # this is only for train\n",
    "            save_img(labels[i], i, output_dir, pid, is_mask=True)\n",
    "    \n",
    "def list_images(directory, ext='jpg|jpeg|bmp|png|tif'):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "            if os.path.isfile(os.path.join(directory, f)) and re.match('([\\w]+\\.(?:' + ext + '))', f)]\n",
    "\n",
    "def preprocess(img, denoise=False):\n",
    "    \"\"\"\n",
    "    Preprocess step after image augmentation, and before feeding into conv net.\n",
    "    \"\"\"\n",
    "    if denoise:\n",
    "        img = cv2.fastNlMeansDenoising(img, h=7)\n",
    "    img = cv2.resize(img, (DataManager.EX_IMG_TARGET_COLS, DataManager.EX_IMG_TARGET_COLS))\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform(img, mask, augment=True):\n",
    "    \"\"\"\n",
    "    Transforms an (img, mask) pair with same augmentation params\n",
    "    \"\"\"\n",
    "    if augment:\n",
    "        pass\n",
    "        #img, mask = augmenter.augment_batch(np.array([img, mask]), same_transform=True)\n",
    "    img = preprocess(img)\n",
    "    mask = preprocess(mask).astype('float32')\n",
    "    return np.array([img]), np.array([mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_zipped_pickle(\"/home/freshpate/mitvalve/data/train.pkl\")\n",
    "test_data = load_zipped_pickle(\"/home/freshpate/mitvalve/data/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 65/65 [00:35<00:00,  1.86it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████| 20/20 [00:21<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(train_data, total=len(train_data)):\n",
    "    imgs = data['video'].T\n",
    "    typeof_ds = data['dataset']\n",
    "    labels = data['label'].T\n",
    "    pacient = data['name']\n",
    "    gen_dataset(imgs, \"train\", pacient, labels, typeof_ds)\n",
    "\n",
    "for data in tqdm(test_data, total=len(test_data)):\n",
    "    imgs = data['video'].T\n",
    "    pacient = data['name']\n",
    "    gen_dataset(imgs, \"test\", pacient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3268"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_images(BASE+'/data/train/expert'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16340"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_images(BASE+'/data/train/amateur'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 132312.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min width: 600 Max width: 1007\n",
      "Min height: 582 Max height: 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_w = 1000\n",
    "max_w = 0\n",
    "min_h = 1000\n",
    "max_h = 0\n",
    "for data in tqdm(test_data, total=len(test_data)):\n",
    "    imgs = data['video'].T\n",
    "    pacient = data['name']\n",
    "    if min_w > imgs.shape[1]:\n",
    "        min_w = imgs.shape[1]\n",
    "    if max_w < imgs.shape[1]:\n",
    "        max_w = imgs.shape[1]\n",
    "    if min_h > imgs.shape[2]:\n",
    "        min_h = imgs.shape[2]\n",
    "    if max_h < imgs.shape[2]:\n",
    "        max_h = imgs.shape[2]\n",
    "\n",
    "print(\"Min width: {} Max width: {}\".format(min_w, max_w))\n",
    "print(\"Min height: {} Max height: {}\".format(min_h, max_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems scaling down for now should be the way to go:\n",
    "https://datascience.stackexchange.com/questions/30396/why-do-we-scale-down-images-before-feeding-them-to-the-network\n",
    "Let's try 224 x 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training amateur images...\n",
      "Done: 0/8170 images\n",
      "Done: 100/8170 images\n",
      "Done: 200/8170 images\n",
      "Done: 300/8170 images\n",
      "Done: 400/8170 images\n",
      "Done: 500/8170 images\n",
      "Done: 600/8170 images\n",
      "Done: 700/8170 images\n",
      "Done: 800/8170 images\n",
      "Done: 900/8170 images\n",
      "Done: 1000/8170 images\n",
      "Done: 1100/8170 images\n",
      "Done: 1200/8170 images\n",
      "Done: 1300/8170 images\n",
      "Done: 1400/8170 images\n",
      "Done: 1500/8170 images\n",
      "Done: 1600/8170 images\n",
      "Done: 1700/8170 images\n",
      "Done: 1800/8170 images\n",
      "Done: 1900/8170 images\n",
      "Done: 2000/8170 images\n",
      "Done: 2100/8170 images\n",
      "Done: 2200/8170 images\n",
      "Done: 2300/8170 images\n",
      "Done: 2400/8170 images\n",
      "Done: 2500/8170 images\n",
      "Done: 2600/8170 images\n",
      "Done: 2700/8170 images\n",
      "Done: 2800/8170 images\n",
      "Done: 2900/8170 images\n",
      "Done: 3000/8170 images\n",
      "Done: 3100/8170 images\n",
      "Done: 3200/8170 images\n",
      "Done: 3300/8170 images\n",
      "Done: 3400/8170 images\n",
      "Done: 3500/8170 images\n",
      "Done: 3600/8170 images\n",
      "Done: 3700/8170 images\n",
      "Done: 3800/8170 images\n",
      "Done: 3900/8170 images\n",
      "Done: 4000/8170 images\n",
      "Done: 4100/8170 images\n",
      "Done: 4200/8170 images\n",
      "Done: 4300/8170 images\n",
      "Done: 4400/8170 images\n",
      "Done: 4500/8170 images\n",
      "Done: 4600/8170 images\n",
      "Done: 4700/8170 images\n",
      "Done: 4800/8170 images\n",
      "Done: 4900/8170 images\n",
      "Done: 5000/8170 images\n",
      "Done: 5100/8170 images\n",
      "Done: 5200/8170 images\n",
      "Done: 5300/8170 images\n",
      "Done: 5400/8170 images\n",
      "Done: 5500/8170 images\n",
      "Done: 5600/8170 images\n",
      "Done: 5700/8170 images\n",
      "Done: 5800/8170 images\n",
      "Done: 5900/8170 images\n",
      "Done: 6000/8170 images\n",
      "Done: 6100/8170 images\n",
      "Done: 6200/8170 images\n",
      "Done: 6300/8170 images\n",
      "Done: 6400/8170 images\n",
      "Done: 6500/8170 images\n",
      "Done: 6600/8170 images\n",
      "Done: 6700/8170 images\n",
      "Done: 6800/8170 images\n",
      "Done: 6900/8170 images\n",
      "Done: 7000/8170 images\n",
      "Done: 7100/8170 images\n",
      "Done: 7200/8170 images\n",
      "Done: 7300/8170 images\n",
      "Done: 7400/8170 images\n",
      "Done: 7500/8170 images\n",
      "Done: 7600/8170 images\n",
      "Done: 7700/8170 images\n",
      "Done: 7800/8170 images\n",
      "Done: 7900/8170 images\n",
      "Done: 8000/8170 images\n",
      "Done: 8100/8170 images\n",
      "Loading training expert images...\n",
      "Done: 0/1634 images\n",
      "Done: 100/1634 images\n",
      "Done: 200/1634 images\n",
      "Done: 300/1634 images\n",
      "Done: 400/1634 images\n",
      "Done: 500/1634 images\n",
      "Done: 600/1634 images\n",
      "Done: 700/1634 images\n",
      "Done: 800/1634 images\n",
      "Done: 900/1634 images\n",
      "Done: 1000/1634 images\n",
      "Done: 1100/1634 images\n",
      "Done: 1200/1634 images\n",
      "Done: 1300/1634 images\n",
      "Done: 1400/1634 images\n",
      "Done: 1500/1634 images\n",
      "Done: 1600/1634 images\n"
     ]
    }
   ],
   "source": [
    "amateur, expert = manager.read_train_images() # just for demonstration purposes, this gets called internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amateur[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training amateur images...\n",
      "Done: 0/8170 images\n",
      "Done: 100/8170 images\n",
      "Done: 200/8170 images\n",
      "Done: 300/8170 images\n",
      "Done: 400/8170 images\n",
      "Done: 500/8170 images\n",
      "Done: 600/8170 images\n",
      "Done: 700/8170 images\n",
      "Done: 800/8170 images\n",
      "Done: 900/8170 images\n",
      "Done: 1000/8170 images\n",
      "Done: 1100/8170 images\n",
      "Done: 1200/8170 images\n",
      "Done: 1300/8170 images\n",
      "Done: 1400/8170 images\n",
      "Done: 1500/8170 images\n",
      "Done: 1600/8170 images\n",
      "Done: 1700/8170 images\n",
      "Done: 1800/8170 images\n",
      "Done: 1900/8170 images\n",
      "Done: 2000/8170 images\n",
      "Done: 2100/8170 images\n",
      "Done: 2200/8170 images\n",
      "Done: 2300/8170 images\n",
      "Done: 2400/8170 images\n",
      "Done: 2500/8170 images\n",
      "Done: 2600/8170 images\n",
      "Done: 2700/8170 images\n",
      "Done: 2800/8170 images\n",
      "Done: 2900/8170 images\n",
      "Done: 3000/8170 images\n",
      "Done: 3100/8170 images\n",
      "Done: 3200/8170 images\n",
      "Done: 3300/8170 images\n",
      "Done: 3400/8170 images\n",
      "Done: 3500/8170 images\n",
      "Done: 3600/8170 images\n",
      "Done: 3700/8170 images\n",
      "Done: 3800/8170 images\n",
      "Done: 3900/8170 images\n",
      "Done: 4000/8170 images\n",
      "Done: 4100/8170 images\n",
      "Done: 4200/8170 images\n",
      "Done: 4300/8170 images\n",
      "Done: 4400/8170 images\n",
      "Done: 4500/8170 images\n",
      "Done: 4600/8170 images\n",
      "Done: 4700/8170 images\n",
      "Done: 4800/8170 images\n",
      "Done: 4900/8170 images\n",
      "Done: 5000/8170 images\n",
      "Done: 5100/8170 images\n",
      "Done: 5200/8170 images\n",
      "Done: 5300/8170 images\n",
      "Done: 5400/8170 images\n",
      "Done: 5500/8170 images\n",
      "Done: 5600/8170 images\n",
      "Done: 5700/8170 images\n",
      "Done: 5800/8170 images\n",
      "Done: 5900/8170 images\n",
      "Done: 6000/8170 images\n",
      "Done: 6100/8170 images\n",
      "Done: 6200/8170 images\n",
      "Done: 6300/8170 images\n",
      "Done: 6400/8170 images\n",
      "Done: 6500/8170 images\n",
      "Done: 6600/8170 images\n",
      "Done: 6700/8170 images\n",
      "Done: 6800/8170 images\n",
      "Done: 6900/8170 images\n",
      "Done: 7000/8170 images\n",
      "Done: 7100/8170 images\n",
      "Done: 7200/8170 images\n",
      "Done: 7300/8170 images\n",
      "Done: 7400/8170 images\n",
      "Done: 7500/8170 images\n",
      "Done: 7600/8170 images\n",
      "Done: 7700/8170 images\n",
      "Done: 7800/8170 images\n",
      "Done: 7900/8170 images\n",
      "Done: 8000/8170 images\n",
      "Done: 8100/8170 images\n",
      "Loading training expert images...\n",
      "Done: 0/1634 images\n",
      "Done: 100/1634 images\n",
      "Done: 200/1634 images\n",
      "Done: 300/1634 images\n",
      "Done: 400/1634 images\n",
      "Done: 500/1634 images\n",
      "Done: 600/1634 images\n",
      "Done: 700/1634 images\n",
      "Done: 800/1634 images\n",
      "Done: 900/1634 images\n",
      "Done: 1000/1634 images\n",
      "Done: 1100/1634 images\n",
      "Done: 1200/1634 images\n",
      "Done: 1300/1634 images\n",
      "Done: 1400/1634 images\n",
      "Done: 1500/1634 images\n",
      "Done: 1600/1634 images\n",
      "Creating train dataset...\n",
      "Saving amateur .npy files done.\n",
      "Saving expert .npy files done.\n"
     ]
    }
   ],
   "source": [
    "manager.create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test images...\n",
      "Done: 0/1572 images\n",
      "Done: 100/1572 images\n",
      "Done: 200/1572 images\n",
      "Done: 300/1572 images\n",
      "Done: 400/1572 images\n",
      "Done: 500/1572 images\n",
      "Done: 600/1572 images\n",
      "Done: 700/1572 images\n",
      "Done: 800/1572 images\n",
      "Done: 900/1572 images\n",
      "Done: 1000/1572 images\n",
      "Done: 1100/1572 images\n",
      "Done: 1200/1572 images\n",
      "Done: 1300/1572 images\n",
      "Done: 1400/1572 images\n",
      "Done: 1500/1572 images\n",
      "Saving test samples...\n",
      "Saving to .npy files done.\n"
     ]
    }
   ],
   "source": [
    "manager.create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = manager.load_train_val_data(\"expert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1\n",
    "\n",
    "def dice(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Average dice across all samples\n",
    "    \"\"\"\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return -dice(y_true, y_pred)\n",
    "\n",
    "# Helper to build a conv -> BN -> relu block\n",
    "def _conv_bn_relu(filters, k_row, k_col, strides=(1, 1)):\n",
    "    def f(input):\n",
    "        conv = Convolution2D(filters=filters, kernel_size=[k_row, k_col],\n",
    "                             strides=strides, kernel_initializer=\"he_normal\",\n",
    "                             padding=\"same\")(input)\n",
    "        norm = BatchNormalization()(conv)\n",
    "        return ELU()(norm)\n",
    "    return f\n",
    "\n",
    "\n",
    "def build_model(optimizer=None):\n",
    "    if optimizer is None:\n",
    "        optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "    inputs = Input((1, DataManager.EX_IMG_TARGET_ROWS, DataManager.EX_IMG_TARGET_COLS), name='main_input')\n",
    "    conv1 = _conv_bn_relu(32, 7, 7)(inputs)\n",
    "    conv1 = _conv_bn_relu(32, 3, 3)(conv1)\n",
    "    pool1 = _conv_bn_relu(32, 2, 2, strides=(2, 2))(conv1)\n",
    "    drop1 = Dropout(0.5)(pool1)\n",
    "\n",
    "    conv2 = _conv_bn_relu(64, 3, 3)(drop1)\n",
    "    conv2 = _conv_bn_relu(64, 3, 3)(conv2)\n",
    "    pool2 = _conv_bn_relu(64, 2, 2, strides=(2, 2))(conv2)\n",
    "    drop2 = Dropout(0.5)(pool2)\n",
    "\n",
    "    conv3 = _conv_bn_relu(128, 3, 3)(drop2)\n",
    "    conv3 = _conv_bn_relu(128, 3, 3)(conv3)\n",
    "    pool3 = _conv_bn_relu(128, 2, 2, strides=(2, 2))(conv3)\n",
    "    drop3 = Dropout(0.5)(pool3)\n",
    "\n",
    "    conv4 = _conv_bn_relu(256, 3, 3)(drop3)\n",
    "    conv4 = _conv_bn_relu(256, 3, 3)(conv4)\n",
    "    pool4 = _conv_bn_relu(256, 2, 2, strides=(2, 2))(conv4)\n",
    "    drop4 = Dropout(0.5)(pool4)\n",
    "\n",
    "    conv5 = _conv_bn_relu(512, 3, 3)(drop4)\n",
    "    conv5 = _conv_bn_relu(512, 3, 3)(conv5)\n",
    "\n",
    "    drop5 = Dropout(0.5)(conv5)\n",
    "    # Using conv to mimic fully connected layer.\n",
    "    aux = Dense(512, kernel_initializer=\"he_normal\", activation='sigmoid')(drop5)\n",
    "    #Convolution2D(filters=1, kernel_size=[drop5.shape[1], drop5.shape[2]],\n",
    "    #                    strides=(1, 1), kernel_initializer=\"he_normal\", activation='sigmoid')(drop5)\n",
    "    \n",
    "    aux = Flatten(name='aux_output')(aux)\n",
    "\n",
    "    up6 = concatenate([UpSampling2D(size=(1,2))(drop5), conv4], axis=-1)\n",
    "    conv6 = _conv_bn_relu(256, 3, 3)(up6)\n",
    "    conv6 = _conv_bn_relu(256, 3, 3)(conv6)\n",
    "    drop6 = Dropout(0.5)(conv6)\n",
    "\n",
    "    up7 = concatenate([UpSampling2D(size=(1,2))(drop6), conv3], axis=3)\n",
    "    conv7 = _conv_bn_relu(128, 3, 3)(up7)\n",
    "    conv7 = _conv_bn_relu(128, 3, 3)(conv7)\n",
    "    drop7 = Dropout(0.5)(conv7)\n",
    "\n",
    "    up8 = concatenate([UpSampling2D(size=(1,2))(drop7), conv2], axis=3)\n",
    "    conv8 = _conv_bn_relu(64, 3, 3)(up8)\n",
    "    conv8 = _conv_bn_relu(64, 3, 3)(conv8)\n",
    "    drop8 = Dropout(0.5)(conv8)\n",
    "\n",
    "    up9 = concatenate([UpSampling2D(size=(1,2))(drop8), conv1],axis=3)\n",
    "    conv9 = _conv_bn_relu(32, 3, 3)(up9)\n",
    "    conv9 = _conv_bn_relu(32, 3, 3)(conv9)\n",
    "    drop9 = Dropout(0.5)(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid', kernel_initializer=\"he_normal\", name='main_output')(drop9)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[conv10, aux])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'main_output': dice_loss, 'aux_output': 'binary_crossentropy'},\n",
    "                  metrics={'main_output': dice, 'aux_output': 'acc'},\n",
    "                  loss_weights={'main_output': 1, 'aux_output': 0.5})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # putting this here just not to brake previous logic, shoud be later removed\n",
    "\n",
    "def add_conv_stage(dim_in, dim_out, kernel_size=3, strides=1, padding='same', use_bias=False, use_BN=False):\n",
    "    if  use_BN:\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=dim_out, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=use_bias),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "            tf.keras.layers.Conv2D(filters=dim_out, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=use_bias),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "        ])\n",
    "    else:\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters=dim_out, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=use_bias),\n",
    "            tf.keras.layers.ReLU(),\n",
    "            tf.keras.layers.Conv2D(filters=dim_out, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=use_bias),\n",
    "            tf.keras.layers.ReLU()\n",
    "        ])\n",
    "    \n",
    "def add_merge_stage(ch_coarse, ch_fine, in_coarse, in_fine, upsample):\n",
    "    conv = tf.keras.layers.Conv2DTranspose(filters=ch_fine, kernel_size=4, strides=2, padding='same', output_padding=1)\n",
    "    \n",
    "def upsample(ch_coarse, ch_fine):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2DTranspose(filters=ch_fine, kernel_size=4, strides=2, padding='same', use_bias=False),\n",
    "        tf.keras.layers.ReLU()\n",
    "    ])\n",
    "\n",
    "class UNet(tf.keras.models.Model):\n",
    "    def __init__(self, use_BN):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1   = add_conv_stage(1, 32, use_BN=use_BN)\n",
    "        self.conv2   = add_conv_stage(32, 64, use_BN=use_BN)\n",
    "        self.conv3   = add_conv_stage(64, 128, use_BN=use_BN)\n",
    "        self.conv4   = add_conv_stage(128, 256, use_BN=use_BN)\n",
    "        self.conv5   = add_conv_stage(256, 512, use_BN=use_BN)\n",
    "\n",
    "        self.conv4m = add_conv_stage(512, 256, use_BN=use_BN)\n",
    "        self.conv3m = add_conv_stage(256, 128, use_BN=use_BN)\n",
    "        self.conv2m = add_conv_stage(128,  64, use_BN=use_BN)\n",
    "        self.conv1m = add_conv_stage( 64,  32, use_BN=use_BN)\n",
    "        \n",
    "        self.conv0 = tf.keras.layers.Conv2D(1, 3, 1, padding='same')\n",
    "        \n",
    "        self.max_pool = tf.keras.layers.MaxPool2D()\n",
    "        \n",
    "        self.upsample54 = upsample(512, 256)\n",
    "        self.upsample43 = upsample(256, 128)\n",
    "        self.upsample32 = upsample(128,  64)\n",
    "        self.upsample21 = upsample(64 ,  32)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        conv1_out = self.conv1(x)\n",
    "        #return self.upsample21(conv1_out)\n",
    "        conv2_out = self.conv2(self.max_pool(conv1_out))\n",
    "        conv3_out = self.conv3(self.max_pool(conv2_out))\n",
    "        conv4_out = self.conv4(self.max_pool(conv3_out))\n",
    "        conv5_out = self.conv5(self.max_pool(conv4_out))\n",
    "\n",
    "        conv5m_out = tf.concat([self.upsample54(conv5_out), conv4_out], -1)\n",
    "        conv4m_out = self.conv4m(conv5m_out)\n",
    "\n",
    "        conv4m_out_ = tf.concat([self.upsample43(conv4m_out), conv3_out], -1)\n",
    "        conv3m_out = self.conv3m(conv4m_out_)\n",
    "\n",
    "        conv3m_out_ = tf.concat([self.upsample32(conv3m_out), conv2_out], -1)\n",
    "        conv2m_out = self.conv2m(conv3m_out_)\n",
    "\n",
    "        conv2m_out_ = tf.concat([self.upsample21(conv2m_out), conv1_out], -1)\n",
    "        conv1m_out = self.conv1m(conv2m_out_)\n",
    "\n",
    "        conv0_out = self.conv0(conv1m_out)\n",
    "\n",
    "        return tf.keras.activations.sigmoid(conv0_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-29 16:57:59.243368: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-29 16:57:59.555208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5433 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "net = UNet(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.build((None, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"u_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 224, 224, 32)      9504      \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 112, 112, 64)      55296     \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 56, 56, 128)       221184    \n",
      "                                                                 \n",
      " sequential_3 (Sequential)   (None, 28, 28, 256)       884736    \n",
      "                                                                 \n",
      " sequential_4 (Sequential)   (None, 14, 14, 512)       3538944   \n",
      "                                                                 \n",
      " sequential_5 (Sequential)   (None, 28, 28, 256)       1769472   \n",
      "                                                                 \n",
      " sequential_6 (Sequential)   (None, 56, 56, 128)       442368    \n",
      "                                                                 \n",
      " sequential_7 (Sequential)   (None, 112, 112, 64)      110592    \n",
      "                                                                 \n",
      " sequential_8 (Sequential)   (None, 224, 224, 32)      27648     \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          multiple                  289       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " sequential_9 (Sequential)   (None, 28, 28, 256)       2097152   \n",
      "                                                                 \n",
      " sequential_10 (Sequential)  (None, 56, 56, 128)       524288    \n",
      "                                                                 \n",
      " sequential_11 (Sequential)  (None, 112, 112, 64)      131072    \n",
      "                                                                 \n",
      " sequential_12 (Sequential)  (None, 224, 224, 32)      32768     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,845,313\n",
      "Trainable params: 9,845,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "net.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = tf.random.uniform(shape=(10000, 224, 224, 1))\n",
    "out = tf.random.uniform(shape=(10000, 224, 224, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 89s 89ms/step - loss: 0.6932 - accuracy: 5.3811e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00ac319880>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(inp, out, batch_size=10, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91541/3735802390.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  net.fit_generator(train_generator, validation_data=val_generator, validation_steps=X_val.shape[0],\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_91541/3735802390.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m net.fit_generator(train_generator, validation_data=val_generator, validation_steps=X_val.shape[0],\n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                      callbacks=[model_checkpoint, reduce_lr, tb], max_queue_size=1000)\n",
      "\u001b[0;32m~/miniconda3/envs/cern/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2014\u001b[0m         \u001b[0;34m'Please use `Model.fit`, which supports generators.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m         stacklevel=2)\n\u001b[0;32m-> 2016\u001b[0;31m     return self.fit(\n\u001b[0m\u001b[1;32m   2017\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cern/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cern/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py\u001b[0m in \u001b[0;36m_get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtransformed\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net.fit_generator(train_generator, validation_data=val_generator, validation_steps=X_val.shape[0],\n",
    "                     steps_per_epoch=X_train.shape[0], epochs=EPOCHS, verbose=2,\n",
    "                     callbacks=[model_checkpoint, reduce_lr, tb], max_queue_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = CustomDataGenerator(X_train, y_train, transform, BATCH_SIZE)\n",
    "\n",
    "# Use fixed samples instead to visualize histograms. There is currently a bug that prevents it\n",
    "# when a val generator is used.\n",
    "# Not aug val samples to keep the eval consistent.\n",
    "val_generator = CustomDataGenerator(X_val, y_val, lambda x, y: transform(x, y, augment=False), BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on model\n"
     ]
    }
   ],
   "source": [
    "run_id = str(datetime.now())\n",
    "model_checkpoint = ModelCheckpoint('./results/net.hdf5', monitor='val_loss', save_best_only=True)\n",
    "tb = TensorBoard(log_dir='./logs/{}'.format(run_id), histogram_freq=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_lr=1e-6)\n",
    "print('Training on model')\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage import transform as tf\n",
    "# import numpy as np\n",
    "# import random\n",
    "\n",
    "# augmenter = ImageAugmenter(DataManager.EX_IMG_TARGET_COLS, DataManager.EX_IMG_TARGET_ROWS,\n",
    "#                            hflip=False, vflip=False,\n",
    "#                            rotation_deg=5,\n",
    "#                            translation_x_px=10,\n",
    "#                            translation_y_px=10)\n",
    "# new_x, new_y = transform(X_train[100], y_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, validation_data=val_generator, validation_steps=X_val.shape[0],\n",
    "                     steps_per_epoch=X_train.shape[0], epochs=EPOCHS, verbose=2,\n",
    "                     callbacks=[model_checkpoint, reduce_lr, tb], max_queue_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
