{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageOps\n",
    "from itertools import repeat\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from data_manager import DataManager, load_img, list_images\n",
    "from data_generator import CustomDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from datetime import datetime\n",
    "import tensorflow\n",
    "from model import UNet\n",
    "import tensorflow.keras.activations as activations\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs:  ['/physical_device:GPU:0', '/physical_device:GPU:1', '/physical_device:GPU:2', '/physical_device:GPU:3']\n",
      "Running on GPU 3\n"
     ]
    }
   ],
   "source": [
    "gpus = tensorflow.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "print(\"Available GPUs: \", [gpu.name for gpu in gpus])\n",
    "gpu_index = 3\n",
    "print(f\"Running on GPU {gpu_index}\")\n",
    "tf.config.experimental.set_visible_devices(devices=gpus[gpu_index], device_type='GPU')\n",
    "tf.config.experimental.set_memory_growth(device=gpus[gpu_index], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = os.getcwd()\n",
    "manager = DataManager()\n",
    "BATCH_SIZE = 6\n",
    "EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "\n",
    "def resize2SquareKeepingAspectRation(img, size, interpolation):\n",
    "    h, w = img.shape[:2]\n",
    "    c = None if len(img.shape) < 3 else img.shape[2]\n",
    "    if h == w: \n",
    "        return cv2.resize(img, (size, size), interpolation)\n",
    "    if h > w: \n",
    "        dif = h\n",
    "    else:     \n",
    "        dif = w\n",
    "    x_pos = int((dif - w)/2.)\n",
    "    y_pos = int((dif - h)/2.)\n",
    "    if c is None:\n",
    "        mask = np.zeros((dif, dif), dtype=img.dtype)\n",
    "        mask[y_pos:y_pos+h, x_pos:x_pos+w] = img[:h, :w]\n",
    "    else:\n",
    "        mask = np.zeros((dif, dif, c), dtype=img.dtype)\n",
    "        mask[y_pos:y_pos+h, x_pos:x_pos+w, :] = img[:h, :w, :]\n",
    "    return cv2.resize(mask, (size, size), interpolation)\n",
    "\n",
    "def grays_to_RGB(img):\n",
    "    # turn image into grayscale RGB\n",
    "    return np.array(Image.fromarray(img).convert(\"RGB\"))\n",
    "\n",
    "def save_img(img, img_idx, path, pid, is_mask=False):\n",
    "    filename = path + '/' + str(pid) + '_' + str(img_idx) \n",
    "    if is_mask: \n",
    "        filename += '_mask.png' \n",
    "        img = np.asarray(img, dtype=\"uint8\") # convert bool mask into uint8 so cv2 doesn't scream\n",
    "    else:\n",
    "        filename += '.png'\n",
    "        img = grays_to_RGB(img)\n",
    "    \n",
    "    cv2.imwrite(filename, img)\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (f\"Creation of the directory {path} failed\", end='\\r')\n",
    "\n",
    "def gen_dataset(imgs, dataset, pid, labels=None, typeof_dataset=None):\n",
    "    output_dir = BASE + '/data/'+dataset+'/'\n",
    "    if os.path.isdir(output_dir) is False:\n",
    "        make_dir(output_dir)\n",
    "    if typeof_dataset is not None: # this is only for train\n",
    "        output_dir+=typeof_dataset #+ '/'\n",
    "        if os.path.isdir(output_dir) is False:\n",
    "            make_dir(output_dir)\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        save_img(img, i, output_dir, pid)\n",
    "        if labels is not None: # this is only for train\n",
    "            save_img(labels[i], i, output_dir, pid, is_mask=True)\n",
    "    \n",
    "def list_images(directory, ext='jpg|jpeg|bmp|png|tif'):\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory)\n",
    "            if os.path.isfile(os.path.join(directory, f)) and re.match('([\\w]+\\.(?:' + ext + '))', f)]\n",
    "\n",
    "def preprocess(img, denoise=False):\n",
    "    \"\"\"\n",
    "    Preprocess step after image augmentation, and before feeding into conv net.\n",
    "    \"\"\"\n",
    "    if denoise:\n",
    "        img = cv2.fastNlMeansDenoising(img, h=7)\n",
    "    \n",
    "    img = resize2SquareKeepingAspectRation(img, DataManager.EX_IMG_TARGET_COLS, cv2.INTER_AREA)\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform(img, mask, augment=True):\n",
    "    \"\"\"\n",
    "    Transforms an (img, mask) pair with same augmentation params\n",
    "    \"\"\"\n",
    "    if augment:\n",
    "        pass\n",
    "        #img, mask = augmenter.augment_batch(np.array([img, mask]), same_transform=True)\n",
    "    img = preprocess(img)\n",
    "    mask = preprocess(mask).astype('float32')\n",
    "    return np.array([img]), np.array([mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = load_zipped_pickle(\"data/train.pkl\")\n",
    "# test_data = load_zipped_pickle(\"data/test.pkl\")\n",
    "# for data in tqdm(train_data, total=len(train_data)):\n",
    "#     imgs = data['video'].T\n",
    "#     typeof_ds = data['dataset']\n",
    "#     labels = data['label'].T\n",
    "#     pacient = data['name']\n",
    "#     gen_dataset(imgs, \"train\", pacient, labels, typeof_ds)\n",
    "\n",
    "# for data in tqdm(test_data, total=len(test_data)):\n",
    "#     imgs = data['video'].T\n",
    "#     pacient = data['name']\n",
    "#     gen_dataset(imgs, \"test\", pacient)\n",
    "# min_w = 1000\n",
    "# max_w = 0\n",
    "# min_h = 1000\n",
    "# max_h = 0\n",
    "# for data in tqdm(test_data, total=len(test_data)):\n",
    "#     imgs = data['video'].T\n",
    "#     pacient = data['name']\n",
    "#     if min_w > imgs.shape[1]:\n",
    "#         min_w = imgs.shape[1]\n",
    "#     if max_w < imgs.shape[1]:\n",
    "#         max_w = imgs.shape[1]\n",
    "#     if min_h > imgs.shape[2]:\n",
    "#         min_h = imgs.shape[2]\n",
    "#     if max_h < imgs.shape[2]:\n",
    "#         max_h = imgs.shape[2]\n",
    "\n",
    "# print(\"Min width: {} Max width: {}\".format(min_w, max_w))\n",
    "# print(\"Min height: {} Max height: {}\".format(min_h, max_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training amateur images...\n",
      "Done: 0/8170 images\n",
      "Done: 100/8170 images\n",
      "Done: 200/8170 images\n",
      "Done: 300/8170 images\n",
      "Done: 400/8170 images\n",
      "Done: 500/8170 images\n",
      "Done: 600/8170 images\n",
      "Done: 700/8170 images\n",
      "Done: 800/8170 images\n",
      "Done: 900/8170 images\n",
      "Done: 1000/8170 images\n",
      "Done: 1100/8170 images\n",
      "Done: 1200/8170 images\n",
      "Done: 1300/8170 images\n",
      "Done: 1400/8170 images\n",
      "Done: 1500/8170 images\n",
      "Done: 1600/8170 images\n",
      "Done: 1700/8170 images\n",
      "Done: 1800/8170 images\n",
      "Done: 1900/8170 images\n",
      "Done: 2000/8170 images\n",
      "Done: 2100/8170 images\n",
      "Done: 2200/8170 images\n",
      "Done: 2300/8170 images\n",
      "Done: 2400/8170 images\n",
      "Done: 2500/8170 images\n",
      "Done: 2600/8170 images\n",
      "Done: 2700/8170 images\n",
      "Done: 2800/8170 images\n",
      "Done: 2900/8170 images\n",
      "Done: 3000/8170 images\n",
      "Done: 3100/8170 images\n",
      "Done: 3200/8170 images\n",
      "Done: 3300/8170 images\n",
      "Done: 3400/8170 images\n",
      "Done: 3500/8170 images\n",
      "Done: 3600/8170 images\n",
      "Done: 3700/8170 images\n",
      "Done: 3800/8170 images\n",
      "Done: 3900/8170 images\n",
      "Done: 4000/8170 images\n",
      "Done: 4100/8170 images\n",
      "Done: 4200/8170 images\n",
      "Done: 4300/8170 images\n",
      "Done: 4400/8170 images\n",
      "Done: 4500/8170 images\n",
      "Done: 4600/8170 images\n",
      "Done: 4700/8170 images\n",
      "Done: 4800/8170 images\n",
      "Done: 4900/8170 images\n",
      "Done: 5000/8170 images\n",
      "Done: 5100/8170 images\n",
      "Done: 5200/8170 images\n",
      "Done: 5300/8170 images\n",
      "Done: 5400/8170 images\n",
      "Done: 5500/8170 images\n",
      "Done: 5600/8170 images\n",
      "Done: 5700/8170 images\n",
      "Done: 5800/8170 images\n",
      "Done: 5900/8170 images\n",
      "Done: 6000/8170 images\n",
      "Done: 6100/8170 images\n",
      "Done: 6200/8170 images\n",
      "Done: 6300/8170 images\n",
      "Done: 6400/8170 images\n",
      "Done: 6500/8170 images\n",
      "Done: 6600/8170 images\n",
      "Done: 6700/8170 images\n",
      "Done: 6800/8170 images\n",
      "Done: 6900/8170 images\n",
      "Done: 7000/8170 images\n",
      "Done: 7100/8170 images\n",
      "Done: 7200/8170 images\n",
      "Done: 7300/8170 images\n",
      "Done: 7400/8170 images\n",
      "Done: 7500/8170 images\n",
      "Done: 7600/8170 images\n",
      "Done: 7700/8170 images\n",
      "Done: 7800/8170 images\n",
      "Done: 7900/8170 images\n",
      "Done: 8000/8170 images\n",
      "Done: 8100/8170 images\n",
      "Loading training expert images...\n",
      "Done: 0/1634 images\n",
      "Done: 100/1634 images\n",
      "Done: 200/1634 images\n",
      "Done: 300/1634 images\n",
      "Done: 400/1634 images\n",
      "Done: 500/1634 images\n",
      "Done: 600/1634 images\n",
      "Done: 700/1634 images\n",
      "Done: 800/1634 images\n",
      "Done: 900/1634 images\n",
      "Done: 1000/1634 images\n",
      "Done: 1100/1634 images\n",
      "Done: 1200/1634 images\n",
      "Done: 1300/1634 images\n",
      "Done: 1400/1634 images\n",
      "Done: 1500/1634 images\n",
      "Done: 1600/1634 images\n",
      "Creating train dataset...\n",
      "Saving amateur .npy files done.\n",
      "Saving expert .npy files done.\n"
     ]
    }
   ],
   "source": [
    "manager.create_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test images...\n",
      "Done: 0/1572 images\n",
      "Done: 100/1572 images\n",
      "Done: 200/1572 images\n",
      "Done: 300/1572 images\n",
      "Done: 400/1572 images\n",
      "Done: 500/1572 images\n",
      "Done: 600/1572 images\n",
      "Done: 700/1572 images\n",
      "Done: 800/1572 images\n",
      "Done: 900/1572 images\n",
      "Done: 1000/1572 images\n",
      "Done: 1100/1572 images\n",
      "Done: 1200/1572 images\n",
      "Done: 1300/1572 images\n",
      "Done: 1400/1572 images\n",
      "Done: 1500/1572 images\n",
      "Saving test samples...\n",
      "Saving to .npy files done.\n"
     ]
    }
   ],
   "source": [
    "manager.create_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = manager.load_train_val_data(\"expert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1470, 1007, 732, 1)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = CustomDataGenerator(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    (224, 224),\n",
    "    preprocess,\n",
    "    batch_size=2,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    #featurewise_center=True,\n",
    "    #featurewise_std_normalization=True,\n",
    "    rotation_range=5, #degrees\n",
    "    width_shift_range=10, #pixels, if <1 fraction\n",
    "    height_shift_range=10,\n",
    "    horizontal_flip=False,\n",
    "    shear_range=5,\n",
    "    rescale=1./255)\n",
    "    #preprocessing_function=preprocess)\n",
    "datagen.generator.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(True)\n",
    "net.build((None, 224, 224, 1))\n",
    "net.summary()\n",
    "run_id = str(datetime.now())\n",
    "model_checkpoint = ModelCheckpoint('./results/net.hdf5', monitor='val_loss', save_best_only=True)\n",
    "tb = TensorBoard(log_dir='./logs/{}'.format(run_id), histogram_freq=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=4, min_lr=1e-6)\n",
    "print('Training on model')\n",
    "net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "735/735 [==============================] - 456s 612ms/step - loss: 807.8589 - accuracy: 0.9911\n",
      "Epoch 2/2\n",
      "153/735 [=====>........................] - ETA: 5:58 - loss: 0.0196 - accuracy: 0.9999"
     ]
    }
   ],
   "source": [
    "net.fit(datagen, epochs=2, steps_per_epoch=len(X_train)//2, callbacks=[model_checkpoint, reduce_lr, tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, validation_data=val_generator, validation_steps=X_val.shape[0],\n",
    "                     steps_per_epoch=X_train.shape[0], epochs=EPOCHS, verbose=2,\n",
    "                     callbacks=[model_checkpoint, reduce_lr, tb], max_queue_size=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
